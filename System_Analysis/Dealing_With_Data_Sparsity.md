At the present state of the Opioid Abuse Predictor, the team is faced with the challenge of sparsity within the data. 
This plethora of zeros within the data has hampered the model's ability to learn meaningful patterns, contributing to poor performance by the model and failure of the loss function to converge. 
In fact, 95% of the data is zeros and on average, each patient has 26 out of 30 feature columns completely empty in their histories. 

The team is considering several options to address the challenge of sparsity within the data:

1. Developing an auto-encoder alongside an RNN, in which they are trained together every iteration. The autoencoder is created and adjusted by evaluation of the predictions of the RNNs
    * Autoencoders can also be used to identify new features from the data, which are specifically useful for RNN performance
    * They can be difficult to interpret.
2. Developing an autoencoder, separate from the RNN, to reduce the dimensionality of our feature matrix, resulting in fewer columns or rows. Ideally, this will produce “new features”, which better explain the label, allowing the model to increase its performance.
    * Autoencoders can also be used to identify new features from the data
    * They can be difficult to interpret.
3. Utilizing other mathematical dimensionality reduction techniques, namely UMAP and tSNE. These techniques also work to identify fewer, more explanatory features for the model. In contrast to autoencoders, however, the newly generated components are not decoded and compared to the original features. We can then train the RNN on the outputs of these techniques.
    * Useful in preserving the local structure of the data and finding fewer, more explanatory features
    * They can be difficult to interpret.
4. Training our RNN on a few Principal Components or Factors, generated by PCA and FA, representing the directions of greatest variability in our data.
    * Easy to interpret and reduce dimensionality
    * Sensitive to the choice of parameters and may suffer in identifying components from large number of zeros
5. Imputing data in place of zeros, through various methods such as basic mean and mediation imputations. 
    * Imputation can be used to fill in missing data, which can improve the performance of the model.
    * Introduce bias to the data, identifying imputation method is challenging
6. Interpolation of data in place of zeros is also a potential option using techniques such as linear and spline interpolation. Unfortunately, implementation of these techniques may still be a challenge given the overwhelming number of zeros in the data.
    * Interpolation can be used to fill in missing data, which can improve the performance of the model.
    * Interpolation bias to the data, identifying imputation method is challenging
7. Train our RNN on columns of indicators where each indicator indicates membership in a group determined by K Nearest Neighbors or K-Means Clustering. 
    * It can also be used to identify important features in the data.
    * Difficult to interpet
8. Utilizing clustering algorithms such as K Nearest neighbors or K-Means Clustering to identify individuals who do and do not have sufficient data for prediction purposes. In this way, we can inform what quantity and type of data is sufficient for prediction. We can then train on individuals deemed to have sufficient data.
    * This approach can help to identify individuals who do not have enough data to be included in the model.
    * Difficult to interpret
9. Aggregating columns by grouping them in relation to each other is another potential solution to sparsity within the dataset. In fact, we pursued this approach, reducing 30 feature columns to just 8, thereby considerably lowering the number of zeros in the data. Unfortunately, this did not improve model performance significantly. Another option is to aggregated rows in addition to aggregating columns, in which one or more visits/events can be grouped together. Ideally, this will increase the density and richness of the data to some extent.
    * Reduce sparsity within dataset and help identify important features in the data.
    * The decision of which columns to aggregate can be potentially be difficult to interpet
10. Creating tails such as 0, 0.25, 0.50, 0.75, and 1 in a dataset with only 0s and 1s can represent a condition, drug, or procedure occurrence building up over time. This would result in fewer number of zeros in the dataset, given that a patient had at least one 1 in a given column.
    * Reduce sparsity within dataset
    * Difficult to interpret as the creation of tails is somewhat artificial. Moreover, implementing tails don’t resolve the issue of completely empty  feature columns for a patient.
11. Denoting 1s as historical occurrences of events rather than current occurrences will allow us to extend 1s to all visits occurrences after the visit occurrence on which the 1 first appeared. Again, this would result in a fewer number of zeros and resolve the issue of “Short Term” memory in regular RNNs.
    * Reduce sparsity within dataset
    * Again, extending 1s don’t resolve the issue of completely empty feature columns for a patient.